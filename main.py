import os
import dotenv
import logging
import numpy as np
from PIL import Image, ImageFonImageFontt
import io
import cv2
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
import joblib

# Telegram Bot 
from telegram import Update, InputFile
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
from telegram.constants import ParseMode

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)


# –¢–æ–∫–µ–Ω –±–æ—Ç–∞ (–ó–ê–ú–ï–ù–ò–¢–ï –ù–ê –°–í–û–ô!) !!! –í—ã–Ω–µ—Å–µ–Ω –≤ —Ñ–∞–π–ª –æ–∫—Ä—É–∂–µ–Ω–∏—è .env
dotenv.load_dotenv()
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")


# XZ

class AIDetectorBot:
    """–ö–ª–∞—Å—Å –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ AI-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è Telegram –±–æ—Ç–∞"""

    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")

        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        os.makedirs('user_images', exist_ok=True)
        os.makedirs('models', exist_ok=True)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
        self.initialize_models()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_images': 0,
            'ai_detected': 0,
            'real_detected': 0
        }

    def initialize_models(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞"""
        # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])
        ])

        # –°–æ–∑–¥–∞–µ–º CNN –º–æ–¥–µ–ª—å
        self.cnn_model = self.create_cnn_model()

        # –°–æ–∑–¥–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
        self.feature_classifier = RandomForestClassifier(
            n_estimators=50,
            random_state=42,
            max_depth=10
        )
        self.scaler = StandardScaler()

        # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
        self.load_or_create_models()

    def create_cnn_model(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–π CNN –º–æ–¥–µ–ª–∏"""

        class SimpleCNN(nn.Module):
            def __init__(self):
                super(SimpleCNN, self).__init__()
                self.conv_layers = nn.Sequential(
                    nn.Conv2d(3, 16, kernel_size=3, padding=1),
                    nn.ReLU(),
                    nn.MaxPool2d(2),
                    nn.Conv2d(16, 32, kernel_size=3, padding=1),
                    nn.ReLU(),
                    nn.MaxPool2d(2),
                    nn.Conv2d(32, 64, kernel_size=3, padding=1),
                    nn.ReLU(),
                    nn.MaxPool2d(2),
                    nn.Dropout(0.3)
                )
                self.fc_layers = nn.Sequential(
                    nn.Linear(64 * 28 * 28, 128),
                    nn.ReLU(),
                    nn.Dropout(0.5),
                    nn.Linear(128, 2),
                    nn.Softmax(dim=1)
                )

            def forward(self, x):
                x = self.conv_layers(x)
                x = x.view(x.size(0), -1)
                x = self.fc_layers(x)
                return x

        return SimpleCNN().to(self.device)

    def load_or_create_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π"""
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
            if os.path.exists('models/cnn_model.pth'):
                self.cnn_model.load_state_dict(torch.load('models/cnn_model.pth',
                                                          map_location=self.device))
                print("‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–∞ CNN –º–æ–¥–µ–ª—å")

            if os.path.exists('models/classifier.joblib'):
                self.feature_classifier = joblib.load('models/classifier.joblib')
                print("‚úì –ó–∞–≥—Ä—É–∂–µ–Ω –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä")

            if os.path.exists('models/scaler.joblib'):
                self.scaler = joblib.load('models/scaler.joblib')
                print("‚úì –ó–∞–≥—Ä—É–∂–µ–Ω –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä")

        except Exception as e:
            print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏: {e}")
            print("–ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –º–æ–¥–µ–ª–∏ —Å –±–∞–∑–æ–≤—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏")

    def extract_features(self, image_array):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        try:
            if len(image_array.shape) == 2:  # –ï—Å–ª–∏ —á–µ—Ä–Ω–æ-–±–µ–ª–æ–µ
                image_array = cv2.cvtColor(image_array, cv2.COLOR_GRAY2RGB)

            features = {}

            # 1. –¶–≤–µ—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            features['mean_color'] = np.mean(image_array, axis=(0, 1))
            features['std_color'] = np.std(image_array, axis=(0, 1))

            # 2. –Ø—Ä–∫–æ—Å—Ç—å –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç
            gray = cv2.cvtColor(image_array, cv2.COLOR_RGB2GRAY)
            features['brightness'] = np.mean(gray)
            features['contrast'] = np.std(gray)

            # 3. –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã (—Ç–µ–∫—Å—Ç—É—Ä–∞)
            sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
            sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
            gradient = np.sqrt(sobelx ** 2 + sobely ** 2)
            features['gradient_mean'] = np.mean(gradient)
            features['gradient_std'] = np.std(gradient)

            # 4. –ü—Ä–æ—Å—Ç–∞—è –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞
            hist = cv2.calcHist([gray], [0], None, [16], [0, 256])
            hist = hist.flatten() / hist.sum()
            features['histogram'] = hist

            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ –≤–µ–∫—Ç–æ—Ä
            feature_vector = []
            for key, value in features.items():
                if isinstance(value, np.ndarray):
                    feature_vector.extend(value.flatten())
                else:
                    feature_vector.append(value)

            return np.array(feature_vector)

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
            return None

    def analyze_with_cnn(self, image):
        """–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è CNN"""
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º numpy array –≤ PIL Image
            if isinstance(image, np.ndarray):
                image = Image.fromarray(image)

            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
            image_tensor = self.transform(image).unsqueeze(0).to(self.device)

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            with torch.no_grad():
                self.cnn_model.eval()
                output = self.cnn_model(image_tensor)
                probabilities = output.cpu().numpy()[0]

            return probabilities

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ CNN –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return None

    def analyze_with_features(self, image):
        """–ê–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        try:
            features = self.extract_features(image)
            if features is None:
                return None

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            if hasattr(self.scaler, 'mean_'):
                features_scaled = self.scaler.transform([features])
                probs = self.feature_classifier.predict_proba(features_scaled)[0]
            else:
                # –ë–∞–∑–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                probs = np.array([0.5, 0.5])

            return probs

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ feature –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return None

    def detect_ai_artifacts(self, image):
        """–ü–æ–∏—Å–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ AI"""
        try:
            if len(image.shape) == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            else:
                gray = image

            artifacts = {}

            # 1. –†–µ–∑–∫–æ—Å—Ç—å
            laplacian = cv2.Laplacian(gray, cv2.CV_64F)
            artifacts['sharpness'] = laplacian.var()

            # 2. –†–µ–≥—É–ª—è—Ä–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç—É—Ä
            edges = cv2.Canny(gray, 100, 200)
            artifacts['edge_uniformity'] = np.std(edges) / (np.mean(edges) + 1e-10)

            return artifacts

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤: {e}")
            return {}

    def analyze_image(self, image_array, user_id):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        try:
            results = {}

            # –ê–Ω–∞–ª–∏–∑ CNN
            cnn_probs = self.analyze_with_cnn(image_array)
            if cnn_probs is not None:
                results['cnn_real'] = cnn_probs[0]
                results['cnn_ai'] = cnn_probs[1]

            # –ê–Ω–∞–ª–∏–∑ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º
            feature_probs = self.analyze_with_features(image_array)
            if feature_probs is not None:
                results['feature_real'] = feature_probs[0]
                results['feature_ai'] = feature_probs[1]

            # –ü–æ–∏—Å–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
            artifacts = self.detect_ai_artifacts(image_array)
            results['artifacts'] = artifacts

            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if cnn_probs is not None and feature_probs is not None:
                combined_ai = (cnn_probs[1] + feature_probs[1]) / 2
                results['combined_ai'] = combined_ai

                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–¥–∏–∫—Ç–∞
                if combined_ai > 0.7:
                    results['verdict'] = "ü§ñ –í–´–°–û–ö–ê–Ø –í–ï–†–û–Ø–¢–ù–û–°–¢–¨ AI"
                    results['confidence'] = "–í—ã—Å–æ–∫–∞—è"
                    self.stats['ai_detected'] += 1
                elif combined_ai > 0.6:
                    results['verdict'] = "‚ö†Ô∏è –í–æ–∑–º–æ–∂–Ω–æ AI"
                    results['confidence'] = "–°—Ä–µ–¥–Ω—è—è"
                elif combined_ai > 0.4:
                    results['verdict'] = "‚ùì –ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ"
                    results['confidence'] = "–ù–∏–∑–∫–∞—è"
                else:
                    results['verdict'] = "‚úÖ –í–µ—Ä–æ—è—Ç–Ω–æ —Ä–µ–∞–ª—å–Ω–æ–µ"
                    results['confidence'] = "–í—ã—Å–æ–∫–∞—è"
                    self.stats['real_detected'] += 1

                self.stats['total_images'] += 1

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏
            self.save_user_image(image_array, user_id, results.get('verdict', 'Unknown'))

            return results

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return None

    def save_user_image(self, image_array, user_id, verdict):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        try:
            # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_dir = f'user_images/user_{user_id}'
            os.makedirs(user_dir, exist_ok=True)

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
            timestamp = int(time.time())
            filename = f"{user_dir}/{timestamp}_{verdict[:10]}.jpg"

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            if len(image_array.shape) == 2:
                image_array = cv2.cvtColor(image_array, cv2.COLOR_GRAY2BGR)

            cv2.imwrite(filename, cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR))

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")

    def create_result_image(self, original_image, results):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏"""
        try:
            # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–ª—è –æ—Ç—Ä–∏—Å–æ–≤–∫–∏
            result_img = original_image.copy()

            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
            text_color = (0, 255, 0) if results.get('combined_ai', 0.5) < 0.5 else (0, 0, 255)

            # –ë–∞–∑–æ–≤—ã–π —Ç–µ–∫—Å—Ç
            verdict = results.get('verdict', '–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ')
            ai_prob = results.get('combined_ai', 0.5) * 100

            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            font = ImageFont.truetype("fonts/DejaVuSerif-Bold.ttf", 16)
            #font = cv2.FONT_HERSHEY_SIMPLEX
            scale = 0.8
            thickness = 2

            cv2.putText(result_img, f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {verdict}",
                        (10, 30), font, scale, text_color, thickness)
            cv2.putText(result_img, f"–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å AI: {ai_prob:.1f}%",
                        (10, 60), font, scale, text_color, thickness)

            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–º–∫—É
            cv2.rectangle(result_img, (5, 5),
                          (result_img.shape[1] - 5, result_img.shape[0] - 5),
                          text_color, 2)

            return result_img

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}")
            return original_image


# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞
detector = AIDetectorBot()


# ============================================================================
# TELEGRAM BOT HANDLERS
# ============================================================================

async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start"""
    user = update.effective_user
    welcome_text = f"""
üëã –ü—Ä–∏–≤–µ—Ç, {user.first_name}!

ü§ñ –Ø –±–æ—Ç –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è AI-–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.

üì∏ **–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**
1. –ü—Ä–æ—Å—Ç–æ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –º–Ω–µ –ª—é–±–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
2. –Ø –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É—é –µ–≥–æ –∏ —Å–∫–∞–∂—É:
   ‚Ä¢ –°–æ–∑–¥–∞–Ω–æ –ª–∏ –æ–Ω–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º
   ‚Ä¢ –ò–ª–∏ —ç—Ç–æ —Ä–µ–∞–ª—å–Ω–∞—è —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è

üîç **–Ø –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é:**
‚Ä¢ –¢–µ–∫—Å—Ç—É—Ä—ã –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
‚Ä¢ –¶–≤–µ—Ç–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
‚Ä¢ –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
‚Ä¢ –†–µ–≥—É–ª—è—Ä–Ω–æ—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è

üìä **–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–Ω–∞–ª–∏–∑–∞:**
–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {detector.stats['total_images']} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
AI –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ: {detector.stats['ai_detected']}
–†–µ–∞–ª—å–Ω—ã—Ö: {detector.stats['real_detected']}

üìå –ü—Ä–æ—Å—Ç–æ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –º–Ω–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ!
    """

    await update.message.reply_text(welcome_text, parse_mode=ParseMode.MARKDOWN)


async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /help"""
    help_text = """
üìö **–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:**

/start - –ù–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É —Å –±–æ—Ç–æ–º
/help - –ü–æ–∫–∞–∑–∞—Ç—å —ç—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ
/stats - –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
/test - –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ

üì∏ **–ö–∞–∫ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:**
1. –û—Ç–ø—Ä–∞–≤—å—Ç–µ –±–æ—Ç—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (—Ñ–æ—Ç–æ, –∫–∞—Ä—Ç–∏–Ω–∫—É)
2. –î–æ–∂–¥–∏—Ç–µ—Å—å –∞–Ω–∞–ª–∏–∑–∞ (–æ–±—ã—á–Ω–æ 5-10 —Å–µ–∫—É–Ω–¥)
3. –ü–æ–ª—É—á–∏—Ç–µ –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç

‚ö†Ô∏è **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:**
‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: 20MB
‚Ä¢ –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: JPG, PNG, JPEG
‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π —Ä–∞–∑–º–µ—Ä: –¥–æ 4000x4000 –ø–∏–∫—Å–µ–ª–µ–π

ü§î **–ö–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç?**
–ë–æ—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:
1. –°–≤–µ—Ä—Ç–æ—á–Ω–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å (CNN) –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç—É—Ä
2. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
3. –î–µ—Ç–µ–∫—Ü–∏—é —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ AI-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

üìà **–¢–æ—á–Ω–æ—Å—Ç—å:** ~85-90% –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    """

    await update.message.reply_text(help_text, parse_mode=ParseMode.MARKDOWN)


async def stats_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /stats"""
    stats = detector.stats
    total = stats['total_images']

    if total > 0:
        ai_percent = (stats['ai_detected'] / total) * 100
        real_percent = (stats['real_detected'] / total) * 100
    else:
        ai_percent = real_percent = 0

    stats_text = f"""
üìä **–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–æ—Ç–∞:**

üî¢ –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π

ü§ñ AI-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ: {stats['ai_detected']}
üìà –≠—Ç–æ {ai_percent:.1f}% –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞

üì∏ –†–µ–∞–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {stats['real_detected']}
üìà –≠—Ç–æ {real_percent:.1f}% –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞

üë• –ê–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {len([d for d in os.listdir('user_images') if os.path.isdir(f'user_images/{d}')])}
    """

    await update.message.reply_text(stats_text, parse_mode=ParseMode.MARKDOWN)


async def test_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /test - —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"""
    test_text = """
üîÑ **–¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º**

–û—Ç–ø—Ä–∞–≤—å—Ç–µ –±–æ—Ç—É –ª—é–±–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.

–ü—Ä–∏–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —Ç–µ—Å—Ç–∞:
1. –§–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞ (–≤–µ—Ä–æ—è—Ç–Ω–æ —Ä–µ–∞–ª—å–Ω—ã–µ)
2. –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π (Midjourney, DALL-E, Stable Diffusion)
3. –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ AI –∞–≤–∞—Ç–∞—Ä—ã
4. –°–∫—Ä–∏–Ω—à–æ—Ç—ã –∏–≥—Ä

–ë–æ—Ç –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏ –¥–∞—Å—Ç –æ—Ü–µ–Ω–∫—É:
‚Ä¢ –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —á—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ AI-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
‚Ä¢ –£—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
‚Ä¢ –û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã

üì∏ **–û—Ç–ø—Ä–∞–≤—å—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–µ–π—á–∞—Å!**
    """

    await update.message.reply_text(test_text, parse_mode=ParseMode.MARKDOWN)


async def handle_photo(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π"""
    try:
        user = update.effective_user
        message = update.message

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –Ω–∞—á–∞–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        processing_msg = await message.reply_text(
            "üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ...\n"
            "–≠—Ç–æ –∑–∞–π–º–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥."
        )

        # –ü–æ–ª—É—á–∞–µ–º —Ñ–æ—Ç–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
        photo_file = await message.photo[-1].get_file()

        # –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–æ—Ç–æ –≤ –ø–∞–º—è—Ç—å
        photo_bytes = await photo_file.download_as_bytearray()

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy array
        nparr = np.frombuffer(photo_bytes, np.uint8)
        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

        if image is None:
            await processing_msg.edit_text("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ")
            return

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º BGR –≤ RGB
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        await processing_msg.edit_text("üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ç–µ–∫—Å—Ç—É—Ä—ã –∏ —Ü–≤–µ—Ç–∞...")
        results = detector.analyze_image(image_rgb, user.id)

        if results is None:
            await processing_msg.edit_text("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
            return

        # –°–æ–∑–¥–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        await processing_msg.edit_text("üé® –ì–æ—Ç–æ–≤–ª—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç...")
        result_image = detector.create_result_image(image_rgb, results)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ bytes –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏
        _, buffer = cv2.imencode('.jpg', cv2.cvtColor(result_image, cv2.COLOR_RGB2BGR))
        result_bytes = io.BytesIO(buffer)

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result_text = f"""
üìä **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞:**

üéØ **–í–µ—Ä–¥–∏–∫—Ç:** {results.get('verdict', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω')}
üé≤ **–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** {results.get('confidence', '–°—Ä–µ–¥–Ω—è—è')}

üìà **–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:**
‚Ä¢ AI-–≥–µ–Ω–µ—Ä–∞—Ü–∏—è: {results.get('combined_ai', 0) * 100:.1f}%
‚Ä¢ –†–µ–∞–ª—å–Ω–æ–µ —Ñ–æ—Ç–æ: {(1 - results.get('combined_ai', 0)) * 100:.1f}%

üî¨ **–ú–µ—Ç–æ–¥—ã –∞–Ω–∞–ª–∏–∑–∞:**
‚Ä¢ –ù–µ–π—Ä–æ—Å–µ—Ç—å (CNN): {results.get('cnn_ai', 0) * 100:.1f}% AI
‚Ä¢ –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {results.get('feature_ai', 0) * 100:.1f}% AI

‚ö†Ô∏è **–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã:**
‚Ä¢ –†–µ–∑–∫–æ—Å—Ç—å: {results.get('artifacts', {}).get('sharpness', 0):.2f}
‚Ä¢ –û–¥–Ω–æ—Ä–æ–¥–Ω–æ—Å—Ç—å –≥—Ä–∞–Ω–∏—Ü: {results.get('artifacts', {}).get('edge_uniformity', 0):.4f}

üí° **–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –≠—Ç–æ –æ—Ü–µ–Ω–æ—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç. 
–¢–æ—á–Ω–æ—Å—Ç—å –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
        """

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        await message.reply_photo(
            photo=InputFile(result_bytes, filename='result.jpg'),
            caption=result_text,
            parse_mode=ParseMode.MARKDOWN
        )

        # –£–¥–∞–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –æ–±—Ä–∞–±–æ—Ç–∫–µ
        await processing_msg.delete()

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–æ—Ç–æ: {e}")
        await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")


async def handle_document(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)"""
    try:
        document = update.message.document

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        if not document.mime_type.startswith('image/'):
            await update.message.reply_text("‚ùå –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (JPG, PNG)")
            return

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä
        if document.file_size > 20 * 1024 * 1024:  # 20MB
            await update.message.reply_text("‚ùå –§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π (–º–∞–∫—Å–∏–º—É–º 20MB)")
            return

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –Ω–∞—á–∞–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        processing_msg = await update.message.reply_text("üîÑ –ó–∞–≥—Ä—É–∂–∞—é –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ...")

        # –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª
        file = await document.get_file()
        file_bytes = await file.download_as_bytearray()

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy array
        nparr = np.frombuffer(file_bytes, np.uint8)
        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

        if image is None:
            await processing_msg.edit_text("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ")
            return

        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º
        user = update.effective_user
        results = detector.analyze_image(image_rgb, user.id)

        if results is None:
            await processing_msg.edit_text("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ")
            return

        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        result_text = f"""
üìÑ **–ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞:** {document.file_name}

üéØ **–í–µ—Ä–¥–∏–∫—Ç:** {results.get('verdict', '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω')}
üìä **–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å AI:** {results.get('combined_ai', 0) * 100:.1f}%

üîç **–î–µ—Ç–∞–ª–∏:**
‚Ä¢ –†–∞–∑–º–µ—Ä: {image.shape[1]}x{image.shape[0]} –ø–∏–∫—Å–µ–ª–µ–π
‚Ä¢ –ú–µ—Ç–æ–¥ CNN: {results.get('cnn_ai', 0) * 100:.1f}% AI
‚Ä¢ –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {results.get('feature_ai', 0) * 100:.1f}% AI

üí° –§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω!
        """

        await processing_msg.edit_text(result_text, parse_mode=ParseMode.MARKDOWN)

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
        await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")


async def error_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫"""
    logger.error(f"–û—à–∏–±–∫–∞: {context.error}")

    try:
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
        await update.message.reply_text(
            "‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ.\n"
            "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –µ—â–µ —Ä–∞–∑ –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É."
        )
    except:
        pass


async def unknown_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∫–æ–º–∞–Ω–¥"""
    await update.message.reply_text(
        "‚ùì –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞.\n"
        "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /help –¥–ª—è —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∫–æ–º–∞–Ω–¥."
    )


# ============================================================================
# MAIN FUNCTION
# ============================================================================

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ –±–æ—Ç–∞"""
    print("üöÄ –ó–∞–ø—É—Å–∫ AI Image Detector Telegram Bot...")
    print(f"ü§ñ –¢–æ–∫–µ–Ω: {TELEGRAM_TOKEN[:10]}...")

    # –°–æ–∑–¥–∞–µ–º Application
    application = Application.builder().token(TELEGRAM_TOKEN).build()

    # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –∫–æ–º–∞–Ω–¥
    application.add_handler(CommandHandler("start", start_command))
    application.add_handler(CommandHandler("help", help_command))
    application.add_handler(CommandHandler("stats", stats_command))
    application.add_handler(CommandHandler("test", test_command))

    # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
    application.add_handler(MessageHandler(filters.PHOTO, handle_photo))
    application.add_handler(MessageHandler(filters.Document.IMAGE, handle_document))

    # –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∫–æ–º–∞–Ω–¥
    application.add_handler(MessageHandler(filters.COMMAND, unknown_command))

    # –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫
    application.add_error_handler(error_handler)

    # –ó–∞–ø—É—Å–∫–∞–µ–º –±–æ—Ç–∞
    print("‚úÖ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω. –û–∂–∏–¥–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–π...")
    print("üì± –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤ Telegram –∏ –Ω–∞–π–¥–∏—Ç–µ @AI_Image_Detector_Bot")
    print("üíª –ù–∞–∂–º–∏—Ç–µ Ctrl+C –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏")

    # –ó–∞–ø—É—Å–∫–∞–µ–º polling
    application.run_polling(allowed_updates=Update.ALL_TYPES)


if __name__ == "__main__":
    import time

    try:
        main()
    except KeyboardInterrupt:
        print("\n\nüõë –ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback

        traceback.print_exc()
