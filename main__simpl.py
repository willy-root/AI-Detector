import os
import dotenv
import logging
import numpy as np
from PIL import Image
import io
import cv2
import torch
import torch.nn as nn
import torchvision.transforms as transforms
import joblib

# –ú—ã —É–±—Ä–∞–ª–∏ –∏–º–ø–æ—Ä—Ç –∏–∑ train_model, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–∫–∏ "No module named..."
# –¢–µ–ø–µ—Ä—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø—Ä–æ–ø–∏—Å–∞–Ω–∞ –ø—Ä—è–º–æ –∑–¥–µ—Å—å.

from telegram import Update, InputFile
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
from telegram.constants import ParseMode

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# –¢–æ–∫–µ–Ω –±–æ—Ç–∞
dotenv.load_dotenv()
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")

# ==========================================
# –í–°–¢–†–û–ï–ù–ù–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ò –§–£–ù–ö–¶–ò–ò
# ==========================================

class SimpleCNN(nn.Module):
    """
    –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏.
    –î—É–±–ª–∏—Ä—É–µ—Ç—Å—è –∑–¥–µ—Å—å, —á—Ç–æ–±—ã –±–æ—Ç –º–æ–≥ —Ä–∞–±–æ—Ç–∞—Ç—å –∞–≤—Ç–æ–Ω–æ–º–Ω–æ –±–µ–∑ —Ñ–∞–π–ª–∞ train_model.py
    """
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv_layers = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Dropout(0.3)
        )
        self.fc_layers = nn.Sequential(
            nn.Linear(64 * 28 * 28, 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, 2), # 2 –∫–ª–∞—Å—Å–∞: Real (0) –∏ AI (1)
            nn.Softmax(dim=1)
        )

    def forward(self, x):
        x = self.conv_layers(x)
        x = x.view(x.size(0), -1)
        x = self.fc_layers(x)
        return x

def extract_features(image_array):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—è—Ä–∫–æ—Å—Ç—å, –∫–æ–Ω—Ç—Ä–∞—Å—Ç, –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã)"""
    try:
        if len(image_array.shape) == 2:
            image_array = cv2.cvtColor(image_array, cv2.COLOR_GRAY2RGB)

        features = {}
        # –Ø—Ä–∫–æ—Å—Ç—å –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç
        gray = cv2.cvtColor(image_array, cv2.COLOR_RGB2GRAY)
        features['brightness'] = np.mean(gray)
        features['contrast'] = np.std(gray)

        # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã (—Ä–µ–∑–∫–æ—Å—Ç—å)
        sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
        sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        gradient = np.sqrt(sobelx**2 + sobely**2)
        features['gradient_mean'] = np.mean(gradient)

        # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞
        hist = cv2.calcHist([gray], [0], None, [16], [0, 256])
        hist = hist.flatten() / (hist.sum() + 1e-6)

        # –°–æ–±–∏—Ä–∞–µ–º –≤–µ–∫—Ç–æ—Ä
        feature_vector = [features['brightness'], features['contrast'], features['gradient_mean']]
        feature_vector.extend(hist)

        return np.array(feature_vector)
    except Exception as e:
        logger.error(f"Error extracting features: {e}")
        return np.zeros(19)

# ==========================================
# –õ–û–ì–ò–ö–ê –ë–û–¢–ê
# ==========================================

class AIDetectorBot:
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")

        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö —Ñ–æ—Ç–æ
        os.makedirs('user_images', exist_ok=True)

        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
        self.load_models()

        # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ (–¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å —Ç–µ–º–∏, —á—Ç–æ –±—ã–ª–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])
        ])

        self.stats = {'total': 0, 'ai': 0, 'real': 0}

    def load_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω—ã—Ö –≤–µ—Å–æ–≤"""
        print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π...")
        try:
            # 1. CNN
            self.cnn_model = SimpleCNN().to(self.device)
            if os.path.exists('models/cnn_model.pth'):
                self.cnn_model.load_state_dict(torch.load('models/cnn_model.pth', map_location=self.device))
                self.cnn_model.eval() # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º –≤ —Ä–µ–∂–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                print("‚úÖ CNN –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            else:
                print("‚ùå –§–∞–π–ª models/cnn_model.pth –Ω–µ –Ω–∞–π–¥–µ–Ω! –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ train_model.py")

            # 2. Random Forest & Scaler
            if os.path.exists('models/classifier.joblib') and os.path.exists('models/scaler.joblib'):
                self.feature_classifier = joblib.load('models/classifier.joblib')
                self.scaler = joblib.load('models/scaler.joblib')
                print("‚úÖ ML –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
            else:
                print("‚ùå –§–∞–π–ª—ã joblib –Ω–µ –Ω–∞–π–¥–µ–Ω—ã! –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ train_model.py")

        except Exception as e:
            print(f"üî• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")

    def analyze_image(self, image_rgb, user_id):
        """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –∞–Ω–∞–ª–∏–∑–∞"""
        results = {}

        try:
            # --- 1. –ê–Ω–∞–ª–∏–∑ CNN ---
            pil_image = Image.fromarray(image_rgb)
            img_tensor = self.transform(pil_image).unsqueeze(0).to(self.device)

            with torch.no_grad():
                outputs = self.cnn_model(img_tensor)
                # outputs = [prob_real, prob_ai]
                probs = outputs.cpu().numpy()[0]

            results['cnn_real'] = float(probs[0])
            results['cnn_ai'] = float(probs[1])

            # --- 2. –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (ML) ---
            features = extract_features(image_rgb)
            # –í–Ω–∏–º–∞–Ω–∏–µ: feature_classifier –æ–∂–∏–¥–∞–µ—Ç 2D –º–∞—Å—Å–∏–≤
            features_scaled = self.scaler.transform(features.reshape(1, -1))
            ml_probs = self.feature_classifier.predict_proba(features_scaled)[0]

            results['feature_real'] = float(ml_probs[0])
            results['feature_ai'] = float(ml_probs[1])

            # --- 3. –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã (OpenCV) ---
            gray = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2GRAY)
            laplacian = cv2.Laplacian(gray, cv2.CV_64F).var()
            results['sharpness'] = laplacian

            # --- –ò—Ç–æ–≥–æ–≤—ã–π –≤–µ—Ä–¥–∏–∫—Ç ---
            # –£—Å—Ä–µ–¥–Ω—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ (–º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –≤–µ—Å–∞)
            final_ai_prob = (results['cnn_ai'] * 0.6) + (results['feature_ai'] * 0.4)
            results['combined_ai'] = final_ai_prob

            if final_ai_prob > 0.65:
                results['verdict'] = "ü§ñ AI GENERATED"
                self.stats['ai'] += 1
            elif final_ai_prob < 0.35:
                results['verdict'] = "üì∏ REAL PHOTO"
                self.stats['real'] += 1
            else:
                results['verdict'] = "‚ùì UNCERTAIN"

            self.stats['total'] += 1

            return results

        except Exception as e:
            logger.error(f"Analysis Failed: {e}")
            return None

    def create_result_vis(self, image_rgb, results):
        """–†–∏—Å—É–µ–º –∫—Ä–∞—Å–∏–≤—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–µ"""
        vis_img = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)

        color = (0, 0, 255) if results['combined_ai'] > 0.5 else (0, 255, 0)
        text = f"{results['verdict']} ({results['combined_ai']*100:.1f}%)"

        # –ü–æ–¥–ª–æ–∂–∫–∞ –ø–æ–¥ —Ç–µ–∫—Å—Ç
        cv2.rectangle(vis_img, (0, 0), (vis_img.shape[1], 60), (0,0,0), -1)
        cv2.putText(vis_img, text, (10, 40), cv2.FONT_HERSHEY_SIMPLEX,
                   1, color, 2)

        return vis_img

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
detector = AIDetectorBot()

# --- Handlers ---

async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("üëã –ü—Ä–∏–≤–µ—Ç! –Ø –≥–æ—Ç–æ–≤. –°–Ω–∞—á–∞–ª–∞ —É–±–µ–¥–∏—Å—å, —á—Ç–æ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä –∑–∞–ø—É—Å—Ç–∏–ª –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π.\n–û—Ç–ø—Ä–∞–≤—å –º–Ω–µ —Ñ–æ—Ç–æ!")

async def handle_photo(update: Update, context: ContextTypes.DEFAULT_TYPE):
    status_msg = await update.message.reply_text("‚è≥ –î—É–º–∞—é...")
    try:
        photo = await update.message.photo[-1].get_file()
        photo_bytes = await photo.download_as_bytearray()
        nparr = np.frombuffer(photo_bytes, np.uint8)
        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        res = detector.analyze_image(img_rgb, update.effective_user.id)

        if res:
            vis_img = detector.create_result_vis(img_rgb, res)

            # –û—Ç–ø—Ä–∞–≤–∫–∞
            _, buffer = cv2.imencode('.jpg', vis_img)
            await update.message.reply_photo(
                photo=io.BytesIO(buffer),
                caption=f"üß† **AI Probability:** {res['combined_ai']*100:.1f}%\n"
                        f"üìä **CNN:** {res['cnn_ai']:.2f} | **ML:** {res['feature_ai']:.2f}",
                parse_mode=ParseMode.MARKDOWN
            )
            await status_msg.delete()
        else:
            await status_msg.edit_text("‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞.")

    except Exception as e:
        logger.error(e)
        await status_msg.edit_text("–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏.")

def main():
    app = Application.builder().token(TELEGRAM_TOKEN).build()
    app.add_handler(CommandHandler("start", start_command))
    app.add_handler(MessageHandler(filters.PHOTO, handle_photo))

    print("üöÄ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω!")
    app.run_polling()

if __name__ == "__main__":
    main()
